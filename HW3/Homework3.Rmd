---
title: "Homework III - Hypothesis Testing"
subtitle: "Bayesian Data Analysis"
author: "Alexander J Ohrt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document:
        code_folding: show
        toc: true
        toc_depth: 3
        theme: yeti
        highlight: textmate
        number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 14, comment = "#>", warning = F)
library(ggplot2)
library(tidyverse)
setwd("/home/ajo/gitRepos/bayesian/HW2")
theme_set(theme_minimal())
```

In this problem we want to decide whether an observed set of data has been generated from a Poisson model or a Geometric model. Remember that the Poisson distribution with parameter $\lambda \in \mathbb{R}^+$ has distribution function

$$
  p_P(x) = \frac{e^{-\lambda}\lambda^x}{x!},
$$

and the Geometric distribution with parameter $\theta \in [0,1]$ has distribution function 

$$
  p_G(x) = \theta(1-\theta)^x.
$$

We observe the number of times an event occurs. We are given the information that the expected value of the counts of this event will be between 4 and 6 with a large probability. Based on this information, we define the following prior distributions 

$$
  \pi(\lambda) = Gamma(\alpha = 100, \beta = 20), 
$$

where $\alpha$ is a shape parameter and $\beta$ is a rate parameter, and 

$$
  \pi(\theta) = Beta(a = 50, b = 235), 
$$

where $a$ is a shape parameter and $b$ is a rate parameter. 

The prior distribution for the Poisson model is relatively easy to choose, since we know that the expected value (and the variance) of a Poisson distributed variable is $\lambda$. Thus, we want $4 < \lambda < 6$ with a large probability. The prior predictive distribution, together with $\pi(\lambda)$, for the Poisson model is plotted below. 

```{r}
N <- 100000
counts <- seq(0,15, length.out = N)
params.for.lambda <- list(alpha = 100, beta = 20)
prior.lambda <- rgamma(N, shape = params.for.lambda$alpha, rate = params.for.lambda$beta)
prior.pred <- rpois(N, lambda = prior.lambda)
ggplot(tibble(prior.pred, prior.lambda), aes(prior.pred)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  geom_density(aes(prior.lambda)) +
  scale_x_continuous(limits = c(0, 15)) + 
  ggtitle("Prior Predictive Distribution in Poisson Model") 

mean(prior.lambda > 4 & prior.lambda < 6)
```

Note that the chosen parameters for the prior lead to a probability of $4 < \lambda < 6$ of `r mean(prior.lambda > 4 & prior.lambda < 6)`.

In order to define the prior distribution for the Geometric model, we use the fact that the expected value of a Geometrically distributed variable is $\frac{1-\theta}{\theta}$. Denote by $k$ the expected value of this random variable, which means that $\theta = \frac{1}{k+1}$. Since we know that $4 < k < 6$ with large probability, equivalently $\frac17 < k < \frac15$ with large probability. Trial and error leads to the choice of the parameters for $\pi(\theta)$.

```{r}
params.for.theta <- list(a = 50, b = 235)
prior.theta <- rbeta(N, shape1 = params.for.theta$a, shape2 = params.for.theta$b)

ggplot(tibble(prior.theta), aes(prior.theta)) +
  geom_density(aes(prior.theta)) +
  scale_x_continuous(limits = c(0, 1)) + 
  ggtitle("Prior Distribution for theta")

sim <- rgeom(N, prior.theta)
ggplot(tibble(sim), aes(sim)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) + 
  scale_x_continuous(limits = c(0, 15)) + 
  ggtitle("Prior Predictive Distribution in Geometric Model")

mean(prior.theta > 1/7 & prior.theta < 1/5)
```

Note that the choice of parameters for the prior leads to a probability of $\frac17 < k < \frac15$ of `r mean(prior.theta > 1/7 & prior.theta < 1/5)`.

The data we observed is given below. 

```{r}
data <- c(1,2,2,8,10)
n <- length(data)
s <- sum(data)
```

Since the Gamma distribution is a conjugate prior for the Poisson and the Beta distribution is a conjugate prior for the Geometric, we know their respective posterior distributions. We calculate the posteriors below. 

```{r}
post.theta <- rbeta(N, shape1 = params.for.theta$a + n, params.for.theta$b + s)
post.lambda <- rgamma(N, shape = params.for.lambda$alpha + s, rate = params.for.lambda$beta + n)

df <- tibble(counts, prior.theta, post.theta,
             prior.lambda, post.lambda)

df2 <- df %>% 
    gather(key = distribution, value = value, -counts) 

df2 %>% 
  filter(distribution %in% c("prior.theta", "post.theta")) %>% 
  ggplot() +
  geom_density(aes(x = value, col = distribution)) +
  scale_x_continuous(limits = c(0, 1)) + 
  ggtitle("Distributions for theta")

df2 %>% 
  filter(distribution %in% c("prior.lambda", "post.lambda")) %>% 
  ggplot() +
  geom_density(aes(x = value, col = distribution)) +
  ggtitle("Distributions for lambda")
```

Next we will decide whether the data has been generated by a Poisson model or a Geometric model. Note that we will not introduce any bias in the expected choice of model, i.e. we are setting the probability of each model to $0.5$.

```{r}
lambda <- sample(prior.lambda, replace = T, size = N)
p.given.pois <- mean(map_dbl(lambda, ~ prod(dpois(data, .x)))) # Weighting the likelihood with the prior. We are simulating an integral. 

theta <- sample(prior.theta, replace = T, size = N)
p.given.geom <- mean(map_dbl(theta, ~ prod(dgeom(data, .x)))) # Weighting the likelihood with the prior. We are simulating an integral. 

# Posterior probabilities for each model. 
(p.poisson.given.y <- (0.5*p.given.pois)/(0.5*(p.given.pois + p.given.geom)))
(p.geom.given.y <- 1 - p.poisson.given.y)

# Bayes Factor.
(BF <- p.geom.given.y/p.poisson.given.y) # I.e. the data gives evidence for the geometric model. 
```

As we can see above, the probability of the Geometric model given the data is the largest, which means that we decide that the data has been generated by a Geometric model. Also, we can see that the Bayes Factor shows that the data gives more evidence in favour of the Geometric model than the Poisson model. 

We are asked to use the Bayesian model averaging approach to plot the posterior predictive distribution for a new future value and compute a point estimate and 95\% credible interval for the prediction. 

The model averaging posterior distribution is given below

```{r}
post.pred.pois <- rpois(N, lambda = post.lambda)
post.pred.geom <- rgeom(N, prob = post.theta)
model.av.post <- p.poisson.given.y*post.pred.pois + p.geom.given.y*post.pred.geom

tibble(model.av.post) %>% 
  ggplot(aes(model.av.post)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_x_continuous(limits = c(0, 15)) + 
  ggtitle("Bayesian Model Averaging Approach Posterior Predictive Distribution")
```

A point estimate for a new future value could be the mean of the Bayesian Model Averaging Approach Posterior Predictive Distribution shown above. The mean is given by

```{r}
mean(model.av.post)
```

Since counts are discrete values, we round this up, which gives 5 as a point estimate of a new future values. 

A 95\% credible interval can be found (for example) via the quantile method, as displayed below.

```{r}
quantile(model.av.post, c(0.025, 0.975))
```

Since counts are discrete values, the quantile based 95\% credible interval is [0, 18] in this case. 
